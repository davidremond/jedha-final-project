{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"if os.path.exists('data'):\n",
    "  shutil.rmtree('data')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d ghostbat101/lung-x-ray-image-clinical-text-dataset -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -n data/lung-x-ray-image-clinical-text-dataset.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remplace les espaces par _ dans les noms des dossiers et fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spaces_with_underscores(directory):\n",
    "    for root, dirs, files in os.walk(directory, topdown=False):\n",
    "        for name in dirs:\n",
    "            new_name = name.replace(' ', '_')\n",
    "            if new_name != name:\n",
    "                old_path = os.path.join(root, name)\n",
    "                new_path = os.path.join(root, new_name)\n",
    "                shutil.move(old_path, new_path)\n",
    "                print(f\"Renamed: {old_path} -> {new_path}\")\n",
    "\n",
    "replace_spaces_with_underscores('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation d'un sample binaire imbalanced avec 20 % des données : Main_dataset_Sample_Binaire_With_Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_malade = 0.875 # >= 0.5 for imbalanced data\n",
    "test_ratio = 0.2  # 20% of data for testing\n",
    "\n",
    "# Chemin vers le dataset principal\n",
    "source_dir = \"/teamspace/studios/this_studio/jedha-final-project/tests/templates/data/Main_dataset\"\n",
    "\n",
    "# Chemin vers le nouveau dataset binaire\n",
    "binary_sample_dir = \"data/Main_dataset_Sample_Binaire_With_Ratio\"\n",
    "binary_sample_test_dir = \"data/Main_dataset_Sample_Binaire_Test\"\n",
    "\n",
    "# Créer les répertoires de sortie pour les classes \"Normal\" et \"Malades\" (Train)\n",
    "os.makedirs(binary_sample_dir, exist_ok=True)\n",
    "normal_dir = os.path.join(binary_sample_dir, \"Normal\")\n",
    "malades_dir = os.path.join(binary_sample_dir, \"Malades\")\n",
    "os.makedirs(normal_dir, exist_ok=True)\n",
    "os.makedirs(malades_dir, exist_ok=True)\n",
    "\n",
    "# Créer les répertoires de sortie pour les classes \"Normal\" et \"Malades\" (Test)\n",
    "os.makedirs(binary_sample_test_dir, exist_ok=True)\n",
    "test_normal_dir = os.path.join(binary_sample_test_dir, \"Normal\")\n",
    "test_malades_dir = os.path.join(binary_sample_test_dir, \"Malades\")\n",
    "os.makedirs(test_normal_dir, exist_ok=True)\n",
    "os.makedirs(test_malades_dir, exist_ok=True)\n",
    "\n",
    "# Liste des classes malades\n",
    "malades_classes = ['Chest_Changes', 'Degenerative_Infectious_Diseases', 'Encapsulated_Lesions',\n",
    "                   'Higher_Density', 'Lower_Density', 'Mediastinal_Changes', 'Obstructive_Pulmonary_Diseases']\n",
    "\n",
    "normal_classes = ['Normal']\n",
    "\n",
    "# Get the number of train and test images for \"Normal\" and \"Malades\"\n",
    "normal_total_images = int(len(os.listdir(os.path.join(source_dir, normal_classes[0])))*0.2)\n",
    "#print(normal_total_images)\n",
    "normal_test_count = int(test_ratio * normal_total_images)\n",
    "#print(normal_test_count)\n",
    "normal_train_count = normal_total_images - normal_test_count\n",
    "\n",
    "malades_total_images = normal_total_images * 7\n",
    "malades_tokeep_images = int(normal_total_images*ratio_malade/(1-ratio_malade))  # pour avoir ratio_malade % du jeu de données qui est malade\n",
    "#print(malades_tokeep_images)\n",
    "malades_test_count = int(test_ratio * malades_tokeep_images)\n",
    "#print(malades_test_count)\n",
    "malades_train_count = malades_tokeep_images - malades_test_count\n",
    "if malades_train_count < normal_train_count*ratio_malade/(1-ratio_malade):\n",
    "    print(f\"pas assez d'images de malades pour ces ratio de malades ({ratio_malade}) et de test ({test_ratio}) : \\\n",
    "        runnez à nouveau la cellule en modifiant ces ratio\")\n",
    "\n",
    "# Process \"Normal\" class\n",
    "for class_name in normal_classes:\n",
    "    class_path = os.path.join(source_dir, class_name)\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Split into train and test\n",
    "    test_images = images[:normal_test_count]\n",
    "    train_images = images[normal_test_count:normal_total_images]\n",
    "\n",
    "    # Copy train images\n",
    "    for img in train_images:\n",
    "        src_path = os.path.join(class_path, img)\n",
    "        dest_path = os.path.join(normal_dir, img)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "    # Copy test images\n",
    "    for img in test_images:\n",
    "        src_path = os.path.join(class_path, img)\n",
    "        dest_path = os.path.join(test_normal_dir, img)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Process \"Malades\" classes\n",
    "for class_name in malades_classes:\n",
    "    class_path = os.path.join(source_dir, class_name)\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Split into train and test\n",
    "    test_images = images[:malades_test_count // len(malades_classes)]\n",
    "    train_images = images[malades_test_count // len(malades_classes):malades_tokeep_images // len(malades_classes)]\n",
    "\n",
    "    # Copy train images\n",
    "    for img in train_images:\n",
    "        src_path = os.path.join(class_path, img)\n",
    "        dest_path = os.path.join(malades_dir, img)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "    # Copy test images\n",
    "    for img in test_images:\n",
    "        src_path = os.path.join(class_path, img)\n",
    "        dest_path = os.path.join(test_malades_dir, img)\n",
    "        shutil.copy(src_path, dest_path)\n",
    "\n",
    "print(\"Les données ont été dupliquées :\")\n",
    "print(f\"- Dans le répertoire {binary_sample_dir} avec les classes 'Normal' et 'Malades' pour l'entraînement.\")\n",
    "print(f\"- Dans le répertoire {binary_sample_test_dir} avec les classes 'Normal' et 'Malades' pour les tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test du nombre d'images dans chaque dossier\n",
    "\n",
    "def count_files_in_each_subdirectory(directory):\n",
    "    try:\n",
    "        # Parcourt le répertoire et ses sous-dossiers\n",
    "        result = {}\n",
    "        for root, _, files in os.walk(directory):\n",
    "            subfolder_name = os.path.relpath(root, directory)  # Nom relatif du sous-dossier\n",
    "            result[subfolder_name] = len(files)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "        return {}\n",
    "\n",
    "#Exemple d'utilisation\n",
    "chemin_du_dossier = \"data/Main_dataset_Sample_Binaire_With_Ratio\"#Main_dataset_Sample_Binaire_Test / Main_dataset_Sample_Binaire_With_Ratio\n",
    "fichiers_par_sous_dossier = count_files_in_each_subdirectory(chemin_du_dossier)\n",
    "\n",
    "#Affiche les résultats\n",
    "for sous_dossier, nombre_de_fichiers in fichiers_par_sous_dossier.items():\n",
    "    print(f\"{sous_dossier} contient {nombre_de_fichiers} fichier(s).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démarrage du tracking MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: >"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLFLOW_SERVER_URI = 'https://david-rem-jedha-final-project-mlops.hf.space'\n",
    "EXPERIMENT_NAME = 'binary' # 'binary' ou 'multi'\n",
    "TRAINER = 'sophie' # Le prénom de la personne qui a exécuté l'entrainement\n",
    "MODEL_TYPE = 'vgg_20_data_aug_sur_mino_test_1epoch' # Le type de modèle utilisé\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "mlflow.start_run(experiment_id = experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing avec data augmentation de la minoritaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de base\n",
    "img_size = (128, 128)  # Taille des images pour redimensionnement\n",
    "batch_size = 32  # Taille des lots pour l'augmentation\n",
    "minority_dir = \"data/Main_dataset_Sample_Binaire_With_Ratio/Normal\"  # Classe minoritaire\n",
    "majority_dir = \"data/Main_dataset_Sample_Binaire_With_Ratio/Malades\"  # Classe majoritaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les images de la classe minoritaire\n",
    "def load_images_from_directory(directory, img_size):\n",
    "    images = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        if os.path.isfile(file_path):  # Vérifie que c'est bien un fichier\n",
    "            img = load_img(file_path, target_size=img_size)  # Charger et redimensionner l'image\n",
    "            img_array = img_to_array(img) / 255.0  # Convertir en tableau NumPy et normaliser\n",
    "            images.append(img_array)\n",
    "    return np.array(images)\n",
    "\n",
    "x_normal = load_images_from_directory(minority_dir, img_size)\n",
    "print(f\"Nombre d'images dans la classe 'Normal' (minoritaire) : {x_normal.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les images de la classe malades\n",
    "x_malades = load_images_from_directory(majority_dir, img_size)\n",
    "print(f\"Nombre d'images dans la classe 'Malades' (majoritaire) : {x_malades.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générateur d'augmentation pour la classe minoritaire\n",
    "img_generator_for_mino = ImageDataGenerator(\n",
    "    rotation_range=90,\n",
    "    brightness_range=(0.5, 1.0),\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'images à générer pour équilibrer parfaitement\n",
    "x_malades.shape[0] - x_normal.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'images à générer\n",
    "num_augmented_samples = (x_malades.shape[0] - x_normal.shape[0]) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Générateur sur classe minoritaire\n",
    "generator = img_generator_for_mino.flow(\n",
    "    x_normal,  # Données de la classe minoritaire\n",
    "    y=None,  # Pas besoin de labels ici\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Génération des images augmentées\n",
    "x_augmented = []\n",
    "for _ in range(num_augmented_samples // batch_size + 1):  # Générer par lots\n",
    "    batch = next(generator)\n",
    "    x_augmented.append(batch)\n",
    "\n",
    "# Convertir en tableau NumPy\n",
    "x_augmented = np.concatenate(x_augmented, axis=0)\n",
    "x_augmented = x_augmented[:num_augmented_samples]  # Limiter au nombre nécessaire\n",
    "print(f\"Nombre d'images augmentées générées : {x_augmented.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels pour chaque classe\n",
    "y_normal = np.zeros((x_normal.shape[0],))  # Classe 0 pour 'Normal'\n",
    "y_malades = np.ones((x_malades.shape[0],))  # Classe 1 pour 'Malades'\n",
    "y_augmented = np.zeros((x_augmented.shape[0],))  # Classe 0 pour les augmentées 'Normal'\n",
    "\n",
    "# Combiner les données\n",
    "x_train = np.concatenate([x_normal, x_malades, x_augmented], axis=0)\n",
    "y_train = np.concatenate([y_normal, y_malades, y_augmented], axis=0)\n",
    "\n",
    "print(f\"Taille finale des données d'entraînement : {x_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mélanger les données et les labels de manière synchronisée\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=42)\n",
    "\n",
    "print(f\"Taille finale des données d'entraînement après mélange : {x_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparer les données en ensembles d'entraînement et de validation\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.3, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Configuration de l'ImageDataGenerator avec preprocessing_function\n",
    "img_generator = ImageDataGenerator(\n",
    "    rescale=1/255., # Rescaling values from [0,255]->[0,1]\n",
    "    rotation_range=180, # Angle range for random image rotation\n",
    "    #width_shift_range=0.1, # Random shift of the image along width axis\n",
    "    #height_shift_range=0.1, # Random shift of the image along height axis\n",
    "    brightness_range=(0.5,1), # Random brightness modification\n",
    "    #shear_range=0.1, # Random distortion of the image\n",
    "    zoom_range=0.1, # Random zoom on the image\n",
    "    #channel_shift_range=50.0, # Random hue modification\n",
    "    horizontal_flip=True, # Randomly flips image horizontally\n",
    "    vertical_flip=True, # Randomly flips image virtically\n",
    "    #fill_mode='wrap',\n",
    "    validation_split=0.2 # Portion of the data that can be saved for validation\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Chargement des ensembles\n",
    "BATCH_SIZE = 64\n",
    "img_size = (128,128)  \n",
    "seed = 42\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Chargement des données  de train avec flow_from_directory de la classe minoritaire\n",
    "img_generator_flow_train_normal = img_generator.flow_from_directory(\n",
    "    directory=\"data/Main_dataset_Sample_Binaire_With_Ratio/Normal\",  # chemin vers les données\n",
    "    target_size=img_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE, # The batch size of the produced batches\n",
    "    shuffle = True ,#Whether to shuffle after all files have been selected once\n",
    "    subset = \"training\"\n",
    ")\n",
    "\n",
    "# Chargement des données de validation avec flow_from_directory de la classe minoritaire\n",
    "img_generator_flow_valid_normal = img_generator.flow_from_directory(\n",
    "    directory=\"data/Main_dataset_Sample_Binaire_With_Ratio/Normal\",  # chemin vers les données\n",
    "    target_size=img_size,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE, # The batch size of the produced batches\n",
    "    shuffle = True, #Whether to shuffle after all files have been selected once\n",
    "    subset = \"validation\"\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 2\n",
    "# CLASSES =7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 09:47:00.343995: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.VGG19(input_shape=(128, 128, 3), \n",
    "                                                     include_top=False,\n",
    "                                                     weights = \"imagenet\",\n",
    "                                                     name=\"vgg19\",\n",
    "                                                     #input_shape=None\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"# Fine-tuning des dernières couches\n",
    "#fine_tune_at = len(base_model.layers) - 20\n",
    "#for layer in base_model.layers[:fine_tune_at]:\n",
    "#    layer.trainable = False\n",
    "fine_tune_at = 2\n",
    "for layer in base_model.layers[-fine_tune_at:]:\n",
    "    layer.trainable = True\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(128, 128, 1)),  # Entrée en niveaux de gris\n",
    "    #tf.keras.layers.Conv2D(3, (1, 1)),  # Conversion 1 -> 3 canaux\n",
    "    base_model,  # pré-entraîné\n",
    "    tf.keras.layers.GlobalMaxPooling2D(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    #tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(CLASSES, activation=\"softmax\")  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a learning rate schedule to decrease the learning rate as we train the model.\n",
    "initial_learning_rate = 0.001#0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "# lr_onplateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.1,\n",
    "#     patience=10,\n",
    "#     verbose=0,\n",
    "#     mode='auto',\n",
    "#     min_delta=0.0001,\n",
    "#     cooldown=0,\n",
    "#     min_lr=0.0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un optimiseur avec le planning du taux d'apprentissage\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    #loss=tf.keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True), #label_smoothing=0.5),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Définir un callback d'early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Surveiller la loss de validation\n",
    "    patience=5,          # Nombre d'époques sans amélioration avant d'arrêter\n",
    "    restore_best_weights=True  # Rétablir les poids du meilleur modèle\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class_indices = img_generator_flow_train.class_indices\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(list(class_indices.values())),\n",
    "    y=img_generator_flow_train.classes\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class_weights = {0: 0.5714285714285714, 1: 12.0}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# imgs, labels = next(iter(img_generator_flow_train))\n",
    "# print(labels)\n",
    "# print(pd.DataFrame(labels).value_counts())\n",
    "class_weights = {}\n",
    "class_weights['0.0'] = (1/60)*(64/2)\n",
    "class_weights['1.0'] = (1/4)*(64/2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    x_train,  # Données d'entraînement\n",
    "    y_train,  # Labels d'entraînement\n",
    "    epochs=EPOCHS,  # Nombre d'époques\n",
    "    batch_size=batch_size,  # Taille des lots\n",
    "    validation_data=(x_val, y_val),  # Validation avec l'ensemble de validation\n",
    "    shuffle=True,  # Mélanger les données à chaque époque\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Entraîner le modèle avec early stopping\n",
    "history = model.fit(\n",
    "    #rgb_train_generator,\n",
    "    img_generator_flow_train,\n",
    "    validation_data=img_generator_flow_valid , #rgb_val_generator,\n",
    "    epochs=EPOCHS,  # Plus d'époques pour laisser l'early stopping décider\n",
    "    #steps_per_epoch=len(img_generator_flow_train),\n",
    "    #validation_steps=len(img_generator_flow_valid),\n",
    "    #callbacks=[early_stopping],\n",
    "    class_weight= class_weights\n",
    ")\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# 5 epochs de plus\n",
    "history = model.fit(\n",
    "    #rgb_train_generator,\n",
    "    img_generator_flow_train,\n",
    "    validation_data=img_generator_flow_valid , #rgb_val_generator,\n",
    "    epochs=EPOCHS,  # Plus d'époques pour laisser l'early stopping décider\n",
    "    #steps_per_epoch=len(img_generator_flow_train),\n",
    "    #validation_steps=len(img_generator_flow_valid),\n",
    "    #callbacks=[early_stopping],\n",
    "    class_weight= class_weights \n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du tracking MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"trainer\", TRAINER) \n",
    "#mlflow.log_param(\"epochs\", EPOCHS) \n",
    "mlflow.log_param(\"model_type\", MODEL_TYPE)\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "# Sauvegarde des métriques par époque\n",
    "history = model.history\n",
    "for epoch in range(len(history.history['loss'])):\n",
    "    mlflow.log_metric('loss', history.history['loss'][epoch], step=epoch)\n",
    "    mlflow.log_metric('accuracy', history.history['accuracy'][epoch], step=epoch)\n",
    "    mlflow.log_metric('val_loss', history.history['val_loss'][epoch], step=epoch)\n",
    "    mlflow.log_metric('val_accuracy', history.history['val_accuracy'][epoch], step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_val)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "#y_true = img_generator_flow_valid.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# prédictions en cherchant le threshold optimal pour la sigmoid (imbalanced categories)\n",
    "\n",
    "seuils = np.arange(0.0, 1.0, 0.01)\n",
    "\n",
    "#Fonction pour calculer la F1-score pour un seuil donné\n",
    "def calculer_f1_score(y_true, y_pred_prob, seuil):\n",
    "    y_pred = (y_pred_prob >= seuil).astype(int)\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "#Trouver le seuil qui maximise la F1-score\n",
    "f1_scores = [calculer_f1_score(y_true, y_pred, seuil) for seuil in seuils]\n",
    "seuil_optimal = seuils[np.argmax(f1_scores)]\n",
    "\n",
    "print(\"Seuil optimal :\", seuil_optimal)\n",
    "print(\"F1-score pour le seuil optimal :\", max(f1_scores))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred)\n",
    "with open(\"classification_report.txt\", \"w\") as file:\n",
    "    file.write(report)\n",
    "mlflow.log_artifact('classification_report.txt', artifact_path=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_pred, output_dict=True)\n",
    "mlflow.log_metric('global_accuracy', report['accuracy'])\n",
    "mlflow.log_metric('macro_avg_precision', report['macro avg']['precision'])\n",
    "mlflow.log_metric('macro_avg_recall', report['macro avg']['recall'])\n",
    "mlflow.log_metric('macro_avg_f1_score', report['macro avg']['f1-score'])\n",
    "mlflow.log_metric('macro_avg_support', report['macro avg']['support'])\n",
    "mlflow.log_metric('weighted_avg_precision', report['weighted avg']['precision'])\n",
    "mlflow.log_metric('weighted_avg_recall', report['weighted avg']['recall'])\n",
    "mlflow.log_metric('weighted_avg_f1_score', report['weighted avg']['f1-score'])\n",
    "mlflow.log_metric('weighted_avg_support', report['weighted avg']['support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_mesure in list(report.items())[:CLASSES]:\n",
    "    for m_name, m_value in class_mesure[1].items():\n",
    "        mlflow.log_metric(m_name, m_value, step=int(float(class_mesure[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Malades\"])\n",
    "disp.plot()\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "mlflow.log_artifact(\"confusion_matrix.png\", artifact_path='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculer la courbe ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#Tracer la courbe ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"roc_curve.png\")\n",
    "mlflow.log_artifact(\"roc_curve.png\", artifact_path='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
