{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras_vis\n",
      "  Downloading keras_vis-0.4.1-py2.py3-none-any.whl.metadata (757 bytes)\n",
      "Requirement already satisfied: h5py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras_vis) (3.12.1)\n",
      "Requirement already satisfied: keras in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras_vis) (3.8.0)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras_vis) (3.8.2)\n",
      "Collecting scikit-image (from keras_vis)\n",
      "  Downloading scikit_image-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: six in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras_vis) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from h5py->keras_vis) (1.26.4)\n",
      "Requirement already satisfied: absl-py in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras->keras_vis) (2.1.0)\n",
      "Requirement already satisfied: rich in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras->keras_vis) (13.9.4)\n",
      "Requirement already satisfied: namex in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras->keras_vis) (0.0.8)\n",
      "Requirement already satisfied: optree in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras->keras_vis) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras->keras_vis) (0.4.1)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from keras->keras_vis) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib->keras_vis) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.11.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image->keras_vis) (1.11.4)\n",
      "Requirement already satisfied: networkx>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image->keras_vis) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from scikit-image->keras_vis) (2.37.0)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->keras_vis)\n",
      "  Downloading tifffile-2025.1.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->keras_vis)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from optree->keras->keras_vis) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras->keras_vis) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich->keras->keras_vis) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras_vis) (0.1.2)\n",
      "Downloading keras_vis-0.4.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading scikit_image-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m186.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2025.1.10-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: tifffile, lazy-loader, scikit-image, keras_vis\n",
      "Successfully installed keras_vis-0.4.1 lazy-loader-0.4 scikit-image-0.25.1 tifffile-2025.1.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install keras_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489ffb71b7834c6490106ad5042a7e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras.src.models.sequential.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "mlops_server_uri = 'https://david-rem-jedha-final-project-mlops.hf.space'\n",
    "model_path = 'models:/lung_7_classes/2'\n",
    "#model_path = \"runs:/73f7af9ab4be4dd1be44b4653bb2bf06/model\"\n",
    "\n",
    "mlflow.set_tracking_uri(mlops_server_uri)\n",
    "keras_model = mlflow.keras.load_model(model_path)\n",
    "\n",
    "# Vérifier si le modèle est bien chargé\n",
    "print(type(keras_model))  # Doit être <class 'keras.engine.functional.Functional'> ou <class 'tensorflow.keras.Model'>\n",
    "\n",
    "base_model = keras_model.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d_190\n",
      "InceptionV3\n",
      "conv2d_191\n",
      "flatten_1\n",
      "dense_1\n",
      "\n",
      "\n",
      "input_layer_2\n",
      "conv2d_96\n",
      "batch_normalization_94\n",
      "activation_94\n",
      "conv2d_97\n",
      "batch_normalization_95\n",
      "activation_95\n",
      "conv2d_98\n",
      "batch_normalization_96\n",
      "activation_96\n",
      "max_pooling2d_4\n",
      "conv2d_99\n",
      "batch_normalization_97\n",
      "activation_97\n",
      "conv2d_100\n",
      "batch_normalization_98\n",
      "activation_98\n",
      "max_pooling2d_5\n",
      "conv2d_104\n",
      "batch_normalization_102\n",
      "activation_102\n",
      "conv2d_102\n",
      "conv2d_105\n",
      "batch_normalization_100\n",
      "batch_normalization_103\n",
      "activation_100\n",
      "activation_103\n",
      "average_pooling2d_9\n",
      "conv2d_101\n",
      "conv2d_103\n",
      "conv2d_106\n",
      "conv2d_107\n",
      "batch_normalization_99\n",
      "batch_normalization_101\n",
      "batch_normalization_104\n",
      "batch_normalization_105\n",
      "activation_99\n",
      "activation_101\n",
      "activation_104\n",
      "activation_105\n",
      "mixed0\n",
      "conv2d_111\n",
      "batch_normalization_109\n",
      "activation_109\n",
      "conv2d_109\n",
      "conv2d_112\n",
      "batch_normalization_107\n",
      "batch_normalization_110\n",
      "activation_107\n",
      "activation_110\n",
      "average_pooling2d_10\n",
      "conv2d_108\n",
      "conv2d_110\n",
      "conv2d_113\n",
      "conv2d_114\n",
      "batch_normalization_106\n",
      "batch_normalization_108\n",
      "batch_normalization_111\n",
      "batch_normalization_112\n",
      "activation_106\n",
      "activation_108\n",
      "activation_111\n",
      "activation_112\n",
      "mixed1\n",
      "conv2d_118\n",
      "batch_normalization_116\n",
      "activation_116\n",
      "conv2d_116\n",
      "conv2d_119\n",
      "batch_normalization_114\n",
      "batch_normalization_117\n",
      "activation_114\n",
      "activation_117\n",
      "average_pooling2d_11\n",
      "conv2d_115\n",
      "conv2d_117\n",
      "conv2d_120\n",
      "conv2d_121\n",
      "batch_normalization_113\n",
      "batch_normalization_115\n",
      "batch_normalization_118\n",
      "batch_normalization_119\n",
      "activation_113\n",
      "activation_115\n",
      "activation_118\n",
      "activation_119\n",
      "mixed2\n",
      "conv2d_123\n",
      "batch_normalization_121\n",
      "activation_121\n",
      "conv2d_124\n",
      "batch_normalization_122\n",
      "activation_122\n",
      "conv2d_122\n",
      "conv2d_125\n",
      "batch_normalization_120\n",
      "batch_normalization_123\n",
      "activation_120\n",
      "activation_123\n",
      "max_pooling2d_6\n",
      "mixed3\n",
      "conv2d_130\n",
      "batch_normalization_128\n",
      "activation_128\n",
      "conv2d_131\n",
      "batch_normalization_129\n",
      "activation_129\n",
      "conv2d_127\n",
      "conv2d_132\n",
      "batch_normalization_125\n",
      "batch_normalization_130\n",
      "activation_125\n",
      "activation_130\n",
      "conv2d_128\n",
      "conv2d_133\n",
      "batch_normalization_126\n",
      "batch_normalization_131\n",
      "activation_126\n",
      "activation_131\n",
      "average_pooling2d_12\n",
      "conv2d_126\n",
      "conv2d_129\n",
      "conv2d_134\n",
      "conv2d_135\n",
      "batch_normalization_124\n",
      "batch_normalization_127\n",
      "batch_normalization_132\n",
      "batch_normalization_133\n",
      "activation_124\n",
      "activation_127\n",
      "activation_132\n",
      "activation_133\n",
      "mixed4\n",
      "conv2d_140\n",
      "batch_normalization_138\n",
      "activation_138\n",
      "conv2d_141\n",
      "batch_normalization_139\n",
      "activation_139\n",
      "conv2d_137\n",
      "conv2d_142\n",
      "batch_normalization_135\n",
      "batch_normalization_140\n",
      "activation_135\n",
      "activation_140\n",
      "conv2d_138\n",
      "conv2d_143\n",
      "batch_normalization_136\n",
      "batch_normalization_141\n",
      "activation_136\n",
      "activation_141\n",
      "average_pooling2d_13\n",
      "conv2d_136\n",
      "conv2d_139\n",
      "conv2d_144\n",
      "conv2d_145\n",
      "batch_normalization_134\n",
      "batch_normalization_137\n",
      "batch_normalization_142\n",
      "batch_normalization_143\n",
      "activation_134\n",
      "activation_137\n",
      "activation_142\n",
      "activation_143\n",
      "mixed5\n",
      "conv2d_150\n",
      "batch_normalization_148\n",
      "activation_148\n",
      "conv2d_151\n",
      "batch_normalization_149\n",
      "activation_149\n",
      "conv2d_147\n",
      "conv2d_152\n",
      "batch_normalization_145\n",
      "batch_normalization_150\n",
      "activation_145\n",
      "activation_150\n",
      "conv2d_148\n",
      "conv2d_153\n",
      "batch_normalization_146\n",
      "batch_normalization_151\n",
      "activation_146\n",
      "activation_151\n",
      "average_pooling2d_14\n",
      "conv2d_146\n",
      "conv2d_149\n",
      "conv2d_154\n",
      "conv2d_155\n",
      "batch_normalization_144\n",
      "batch_normalization_147\n",
      "batch_normalization_152\n",
      "batch_normalization_153\n",
      "activation_144\n",
      "activation_147\n",
      "activation_152\n",
      "activation_153\n",
      "mixed6\n",
      "conv2d_160\n",
      "batch_normalization_158\n",
      "activation_158\n",
      "conv2d_161\n",
      "batch_normalization_159\n",
      "activation_159\n",
      "conv2d_157\n",
      "conv2d_162\n",
      "batch_normalization_155\n",
      "batch_normalization_160\n",
      "activation_155\n",
      "activation_160\n",
      "conv2d_158\n",
      "conv2d_163\n",
      "batch_normalization_156\n",
      "batch_normalization_161\n",
      "activation_156\n",
      "activation_161\n",
      "average_pooling2d_15\n",
      "conv2d_156\n",
      "conv2d_159\n",
      "conv2d_164\n",
      "conv2d_165\n",
      "batch_normalization_154\n",
      "batch_normalization_157\n",
      "batch_normalization_162\n",
      "batch_normalization_163\n",
      "activation_154\n",
      "activation_157\n",
      "activation_162\n",
      "activation_163\n",
      "mixed7\n",
      "conv2d_168\n",
      "batch_normalization_166\n",
      "activation_166\n",
      "conv2d_169\n",
      "batch_normalization_167\n",
      "activation_167\n",
      "conv2d_166\n",
      "conv2d_170\n",
      "batch_normalization_164\n",
      "batch_normalization_168\n",
      "activation_164\n",
      "activation_168\n",
      "conv2d_167\n",
      "conv2d_171\n",
      "batch_normalization_165\n",
      "batch_normalization_169\n",
      "activation_165\n",
      "activation_169\n",
      "max_pooling2d_7\n",
      "mixed8\n",
      "conv2d_176\n",
      "batch_normalization_174\n",
      "activation_174\n",
      "conv2d_173\n",
      "conv2d_177\n",
      "batch_normalization_171\n",
      "batch_normalization_175\n",
      "activation_171\n",
      "activation_175\n",
      "conv2d_174\n",
      "conv2d_175\n",
      "conv2d_178\n",
      "conv2d_179\n",
      "average_pooling2d_16\n",
      "conv2d_172\n",
      "batch_normalization_172\n",
      "batch_normalization_173\n",
      "batch_normalization_176\n",
      "batch_normalization_177\n",
      "conv2d_180\n",
      "batch_normalization_170\n",
      "activation_172\n",
      "activation_173\n",
      "activation_176\n",
      "activation_177\n",
      "batch_normalization_178\n",
      "activation_170\n",
      "mixed9_0\n",
      "concatenate_2\n",
      "activation_178\n",
      "mixed9\n",
      "conv2d_185\n",
      "batch_normalization_183\n",
      "activation_183\n",
      "conv2d_182\n",
      "conv2d_186\n",
      "batch_normalization_180\n",
      "batch_normalization_184\n",
      "activation_180\n",
      "activation_184\n",
      "conv2d_183\n",
      "conv2d_184\n",
      "conv2d_187\n",
      "conv2d_188\n",
      "average_pooling2d_17\n",
      "conv2d_181\n",
      "batch_normalization_181\n",
      "batch_normalization_182\n",
      "batch_normalization_185\n",
      "batch_normalization_186\n",
      "conv2d_189\n",
      "batch_normalization_179\n",
      "activation_181\n",
      "activation_182\n",
      "activation_185\n",
      "activation_186\n",
      "batch_normalization_187\n",
      "activation_179\n",
      "mixed9_1\n",
      "concatenate_3\n",
      "activation_187\n",
      "mixed10\n",
      "\n",
      "\n",
      "mixed10\n"
     ]
    }
   ],
   "source": [
    "for layer in keras_model.layers:\n",
    "    print(layer.name)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "for layer in keras_model.layers[1].layers:\n",
    "    print(layer.name)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "last_conv_layer_name = keras_model.layers[1].layers[-1].name\n",
    "print(last_conv_layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InceptionV3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">21,802,784</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">57,351</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_94 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m6\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ InceptionV3 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m21,802,784\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │        \u001b[38;5;34m57,351\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,852,860</span> (90.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,852,860\u001b[0m (90.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,992,717</span> (7.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,992,717\u001b[0m (7.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,867,424</span> (75.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m19,867,424\u001b[0m (75.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,992,719</span> (7.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,992,719\u001b[0m (7.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Conv2D name=conv2d_191, built=True>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras_model.layers[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Grad-CAM algorithm \n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (224, 224, 1)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    print(array.shape)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 224, 224, 1)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    print(array.shape)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(\n",
    "    img_array, base_model, model, last_conv_layer_name, classifier_layer_names):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = base_model.get_layer(last_conv_layer_name)\n",
    "    first_conv_layer = model.get_layer('conv2d_94')\n",
    "    #last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    #print(last_conv_layer)\n",
    "    first_conv_layer_model = tf.keras.Model(base_model.inputs, first_conv_layer.output)\n",
    "    last_conv_layer_model = tf.keras.Model(first_conv_layer_model.inputs, last_conv_layer.output)\n",
    "    #last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n",
    "    #print(last_conv_layer_model)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    print(classifier_layer_names[0])\n",
    "    first_input = tf.keras.Input(shape=img_array.shape)\n",
    "    #classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:]) #last_conv_layer.output.shape[1:] #img_array.shape\n",
    "    x = first_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        print(layer_name)\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = tf.keras.Model(first_input, x) #classifier_input\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        #last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        last_conv_layer_input = first_conv_layer_model(img_array)\n",
    "        last_conv_layer_output = last_conv_layer_model(last_conv_layer_input)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 1)\n",
      "['conv2d_94', 'InceptionV3', 'flatten', 'dense']\n",
      "mixed10\n",
      "conv2d_94\n",
      "conv2d_94\n",
      "InceptionV3\n",
      "flatten\n",
      "dense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_layer']\n",
      "Received: inputs=Tensor(shape=(128, 128, 1))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input [[[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n ...\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]]. Expected shape (None, 128, 128, 3), but input has incompatible shape (128, 128, 1)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=array([[[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       ...,\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]]], dtype=uint8)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m last_conv_layer_name \u001b[38;5;241m=\u001b[39m keras_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_conv_layer_name)\n\u001b[0;32m---> 15\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mmake_gradcam_heatmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m      \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeras_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_conv_layer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_layer_names\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;66;43;03m#array, base_model, keras_model, \"conv2d_191\", classifier_layer_names\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 44\u001b[0m, in \u001b[0;36mmake_gradcam_heatmap\u001b[0;34m(img_array, base_model, model, last_conv_layer_name, classifier_layer_names)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Then, we compute the gradient of the top predicted class for our input image\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# with respect to the activations of the last conv layer\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Compute activations of the last conv layer and make the tape watch it\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#last_conv_layer_output = last_conv_layer_model(img_array)\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     last_conv_layer_input \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_conv_layer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     last_conv_layer_output \u001b[38;5;241m=\u001b[39m last_conv_layer_model(last_conv_layer_input)\n\u001b[1;32m     46\u001b[0m     tape\u001b[38;5;241m.\u001b[39mwatch(last_conv_layer_output)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/models/functional.py:272\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input [[[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n ...\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]\n\n [[0.]\n  [0.]\n  [0.]\n  ...\n  [0.]\n  [0.]\n  [0.]]]. Expected shape (None, 128, 128, 3), but input has incompatible shape (128, 128, 1)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=array([[[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       ...,\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]],\n\n       [[0],\n        [0],\n        [0],\n        ...,\n        [0],\n        [0],\n        [0]]], dtype=uint8)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "#array = get_img_array(\"./image_0007.jpg\", (224, 224, 1))\n",
    "img = cv2.imread(\"./image_0007.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.resize(img, (128, 128)) # Ajustez selon votre modèle\n",
    "array = np.expand_dims(img, axis=-1) # Ajustez selon votre modèle\n",
    "#array = np.expand_dims(np.expand_dims(img, axis=-1), axis=0)\n",
    "#img_color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "#array = np.expand_dims(img_color, axis=0) \n",
    "print(array.shape)\n",
    "\n",
    "classifier_layer_names = [layer.name for layer in keras_model.layers]\n",
    "print(classifier_layer_names)\n",
    "last_conv_layer_name = keras_model.layers[1].layers[-1].name\n",
    "print(last_conv_layer_name)\n",
    "\n",
    "heatmap = make_gradcam_heatmap(\n",
    "      array, base_model, keras_model, last_conv_layer_name, classifier_layer_names\n",
    "      #array, base_model, keras_model, \"conv2d_191\", classifier_layer_names\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 1)\n",
      "conv2d_190\n",
      "InceptionV3\n",
      "conv2d_191\n",
      "flatten_1\n",
      "dense_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor_682']\n",
      "Received: inputs=Tensor(shape=(128, 128, 1))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"InceptionV3\" is incompatible with the layer: expected shape=(None, 128, 128, 3), found shape=(128, 128, 1, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(128, 128, 1, 1), dtype=float32)\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#from PIL import Image\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#img = Image.open(\"./image_0007.jpg\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#img_resized = img.resize((128, 128))\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mapply_gradcam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./image_0007.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeras_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv2d_191\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 26\u001b[0m, in \u001b[0;36mapply_gradcam\u001b[0;34m(image_path, keras_model, layer_name)\u001b[0m\n\u001b[1;32m     23\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Ajustez selon votre modèle, si besoin de (x,x,3) : np.expand_dims(np.expand_dims(img, axis=-1), axis=0)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(img_array\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 26\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mget_gradcam_heatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeras_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(heatmap, (img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     29\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8(\u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m heatmap)\n",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m, in \u001b[0;36mget_gradcam_heatmap\u001b[0;34m(keras_model, img_array, layer_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m grad_model \u001b[38;5;241m=\u001b[39m Model([keras_model2\u001b[38;5;241m.\u001b[39minput], [keras_model\u001b[38;5;241m.\u001b[39mget_layer(layer_name)\u001b[38;5;241m.\u001b[39moutput, keras_model2\u001b[38;5;241m.\u001b[39moutput])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 11\u001b[0m     conv_outputs, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m predictions[:, tf\u001b[38;5;241m.\u001b[39margmax(predictions[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m     13\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, conv_outputs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/keras/src/layers/input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"InceptionV3\" is incompatible with the layer: expected shape=(None, 128, 128, 3), found shape=(128, 128, 1, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(128, 128, 1, 1), dtype=float32)\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "#from PIL import Image\n",
    "#img = Image.open(\"./image_0007.jpg\")\n",
    "#img_resized = img.resize((128, 128))\n",
    "\n",
    "\n",
    "apply_gradcam(\"./image_0007.jpg\", keras_model, \"conv2d_191\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
