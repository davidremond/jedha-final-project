{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle tensorflow plotly -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d ghostbat101/lung-x-ray-image-clinical-text-dataset\n",
    "!unzip -n lung-x-ray-image-clinical-text-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démarrage du tracking MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_SERVER_URI = 'https://david-rem-jedha-final-project-mlops.hf.space'\n",
    "EXPERIMENT_NAME = 'multi' # 'binary' ou 'multi'\n",
    "TRAINER = 'david' # Le prénom de la personne qui a exécuté l'entrainement\n",
    "MODEL_TYPE = 'baseline' # Le type de modèle utilisé\n",
    "EPOCHS = 1\n",
    "IMAGE_PATH = 'jedha-final-project/tests/baseline/binary/Main dataset'\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_SERVER_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "mlflow.start_run(experiment_id = experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Mettre ici la préparation des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sauvegarde du tracking MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"trainer\", TRAINER) \n",
    "mlflow.log_param(\"epochs\", EPOCHS) \n",
    "mlflow.log_param(\"model_type\", MODEL_TYPE)\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "# Sauvegarde des métriques par époque\n",
    "history = model.history\n",
    "for epoch in range(len(history.history['loss'])):\n",
    "    mlflow.log_metric('loss', history.history['loss'][epoch], step=epoch)\n",
    "    mlflow.log_metric('accuracy', history.history['accuracy'][epoch], step=epoch)\n",
    "    mlflow.log_metric('val_loss', history.history['val_loss'][epoch], step=epoch)\n",
    "    mlflow.log_metric('val_accuracy', history.history['val_accuracy'][epoch], step=epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_generator)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = val_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_pred, y_true, output_dict=True)\n",
    "mlflow.log_metric('global_accuracy', report['accuracy'])\n",
    "mlflow.log_metric('macro_avg_precision', report['macro avg']['precision'])\n",
    "mlflow.log_metric('macro_avg_recall', report['macro avg']['recall'])\n",
    "mlflow.log_metric('macro_avg_f1_score', report['macro avg']['f1-score'])\n",
    "mlflow.log_metric('macro_avg_support', report['macro avg']['support'])\n",
    "mlflow.log_metric('weighted_avg_precision', report['weighted avg']['precision'])\n",
    "mlflow.log_metric('weighted_avg_recall', report['weighted avg']['recall'])\n",
    "mlflow.log_metric('weighted_avg_f1_score', report['weighted avg']['f1-score'])\n",
    "mlflow.log_metric('weighted_avg_support', report['weighted avg']['support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_mesure in list(report.items())[:CLASSES]:\n",
    "    for m_name, m_value in class_mesure[1].items():\n",
    "        mlflow.log_metric(m_name, m_value, step=int(class_mesure[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=val_generator.class_indices.keys())\n",
    "disp.plot()\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "mlflow.log_artifact(\"confusion_matrix.png\", artifact_path='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
